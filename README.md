# effective-train

Repository containing my proposed stack for the code test given by Knowit.

A customer wants to analyze data from swapi.dev on a daily basis, make a rudimentary stack for downloading data from swapi.dev and making it available for analysis.

## Solution

The proposed data pipeline is build using Dagster and PostgreSQL for final storage.

The data pipeline consists of three stages for each of the six data entities (films, people, planets, species, starships and vehicles):
1. Extract data using HTTP request and store (without modification) on disk as json.
2. Validate and drop faulty records.
3. Write data to PostgreSQL table for final storage.

The pipeline is scheduled to run daily at 12:00 AM UTC.

### Limitations
- No relations between final tables in database.
- No deployment has been implemented (use `dagster dev`) to start webserver locally. 
- Dagster run id is used to handle data from many runs. Would have liked to use a real Data Warehouse, but due to lack of time this was my goto workaround.
- Only validating data and dropping, no cleaning has been implemented.


### Developed and tested using

- Ubuntu 22.04
- Python 3.10.12


## Getting started

### Python

Create a virtual Python environment, activate and run:

    pip install .

### PostgreSQL

Start PostgreSQL instance using the following command

    docker compose -f docker-compose-pg-only.yml up

Create database `swapi` and schemas by executing

    python setup_db.py

### Dagster

Start Dagster webserver by running

    dagster dev

Open Dagster UI (default is localhost:3000) and manually trigger a run of the all_assets_job.

- The get_* assets will produce metadata showing how many items was collected.
- The validate_* assets will produce metadata showing how many items was ok and not ok (i.e. keept or removed).
- The insert_* assets will show the shape of the data which was written to the table. 

## Query final tables

Connect to the database instance (example when using provided docker compose yml)

    docker exec -it effective-train-db-1 psql -U postgres -d swapi

To extract information from data generated by a specific run you have to know the dagster run id.

Example: min, avg, max and total height

    SELECT MIN(height) as height_min, AVG(height) AS height_avg, MAX(height) as height_max, SUM(height) as height_total FROM people;

> NOTE: If relations was to be included one could have analyzed differences based on e.g. homeworld etc.

